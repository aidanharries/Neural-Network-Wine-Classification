{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network: Wine Classification**"
      ],
      "metadata": {
        "id": "ZcfAvN15JDcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "In this project, I dive into the basics of Neural Networks using PyTorch. I will be using the **Wine** dataset, from **sklearn**, aiming to predict wine classes based on various features. This is my first project in this domain, and I am excited to share my learnings and insights.\n",
        "\n",
        "Before diving into the code, let's revist our task and dataset to ensure clarity. This is the Wine dataset, which comprises various chemical consituents of wines grown in the same region in Italy but derived from three different cultivars (types of grape plants). The dataset provides 13 features based on these chemical constituents and a target label representing the wine's class. The classes are as follows:\n",
        "\n",
        "- Class 0: Wine from the first cultivar\n",
        "- Class 1: Wine from the second cultivar\n",
        "- Class 2: Wine from the third cultivar\n",
        "\n",
        "Traditionally, this would be a classification task where the goal is to assign one of the discrete class labels (0, 1, or 2) based on the wine's chemical composition. However, in this approach, I have converted the class labels to floating-point numbers to frame this as a regression task."
      ],
      "metadata": {
        "id": "dZdPLvz5Xp5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup and Data Loading**\n",
        "\n",
        "First, we need to import the necessary libraries and load the data.\n"
      ],
      "metadata": {
        "id": "Zf02rIZP6mMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target.astype(float)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "BAcDR3cKn_Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the Wine dataset is loaded and split into training and test sets. The features are also standardized, which is a crucial preprocessing step to ensure that all features contribute equally to the model training."
      ],
      "metadata": {
        "id": "4cEnNrQPy5U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Conversion to PyTorch Tensors**\n",
        "\n",
        "Next, the data is converted to PyTorch tensors, which are the primary data structure used in PyTorch for storing multidimensional arrays."
      ],
      "metadata": {
        "id": "BvROFvHpzNt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
      ],
      "metadata": {
        "id": "lvueK_qgzIIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Neural Network Model**\n",
        "\n",
        "Now, let's define the neural network model. We will start with a simple two-layer model."
      ],
      "metadata": {
        "id": "4Vnhag2s0oue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 10\n",
        "output_size = 1\n",
        "model = NeuralNet(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "M19-wQLA0yle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **'NeuralNet'** class defines a simple neural network with two fully connected layers, created to perform a regression task on the wine dataset.\n",
        "\n",
        "- **Layer 1**: A linear layer that transforms the input features into an intermediate representation of size 10.\n",
        "- **Layer 2**: Another linear layer that takes the intermediate representation and outputs a single continuous value.\n",
        "\n",
        "The forward pass of the network is defined to take an input, pass it through both layers sequentially, and produce the final output. The model is initialized with the size of the input features, an arbitrary hidden size of 10, and an output size of 1. This architecture provides a basic yet effective introduction to neural networks."
      ],
      "metadata": {
        "id": "IoxrRepA6eBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**\n",
        "\n",
        "Next, we will train the model using Mean Squared Error as the loss function and Adam as the optimizer."
      ],
      "metadata": {
        "id": "rrFaXTep-Lda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters and loss function\n",
        "learning_rate = 0.01\n",
        "epochs = 5000\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 500 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUw94qO_8SV1",
        "outputId": "d4401872-5f0c-4695-ea3e-0c56523b0936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [500/5000], Loss: 0.0580\n",
            "Epoch [1000/5000], Loss: 0.0580\n",
            "Epoch [1500/5000], Loss: 0.0580\n",
            "Epoch [2000/5000], Loss: 0.0580\n",
            "Epoch [2500/5000], Loss: 0.0580\n",
            "Epoch [3000/5000], Loss: 0.0580\n",
            "Epoch [3500/5000], Loss: 0.0580\n",
            "Epoch [4000/5000], Loss: 0.0580\n",
            "Epoch [4500/5000], Loss: 0.0582\n",
            "Epoch [5000/5000], Loss: 0.0580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Hyperparameters**: The learning rate is set to 0.01 and a total of 5000 training epochs was decided. The learning rate controls the step size during optimization, while the number of epochs defines how many times the learning algorithm will work through the entire training dataset.\n",
        "\n",
        "- **Loss Function**: The MSE loss function measures the average squared difference between the estimated values and the actual value, making it suitable for regression tasks.\n",
        "\n",
        "- **Optimizer**: The Adam optimizer is employed, a popular choice due to its adaptiveness in handling sparse gradients on noisy problems.\n",
        "\n",
        "- **Training Loop**: In each epoch of the training loop, the model performs the following steps:\n",
        "  \n",
        "    1. **Forward Pass**: Compute the predicted values.\n",
        "    2. **Calculate Loss**: Compute the loss using the predicted values and actual labels.\n",
        "    3. **Backward Pass**: Compute the gradient of the loss with respect to the parameters.\n",
        "    4. **Update Weights**: Adjust the weights to minimize the loss.\n",
        "\n",
        "The loss outputs every 500 epochs to track the training progress and ensure that the model is learning effectively."
      ],
      "metadata": {
        "id": "-hvNETUOEiNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation**\n",
        "\n",
        "Now, let's evaluate the model on the test data."
      ],
      "metadata": {
        "id": "dmjQbxtNHvw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    test_loss = criterion(test_outputs, y_test_tensor)\n",
        "    print(f\"Test Loss: {test_loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa3gW8lsHhW8",
        "outputId": "e68e57fb-08ca-4876-c333-304d497dd773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the evaluation phase, the gradient computations are turned off, as we are not looking to update the model's weights any further. This is done using the context manager **'torch.no_grad()'**, which results in less memory consumption and speeds up computations.\n",
        "\n",
        "Within this context, the test data, **'X_test_tensor'**, is passed through the model to get predictions. We then calculate and print the Mean Squared Error Loss between these predictions and the actual values, **'y_test_tensor'**, resulting in a test loss of 0.0685. This loss gives a quantitative measure of how well the model is performing; the lower the loss, the better the model's predictions are."
      ],
      "metadata": {
        "id": "TL2W_IG3JHAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predictions**\n",
        "\n",
        "Finally, let's print out some of the model's predictions along with the true values."
      ],
      "metadata": {
        "id": "iL1R-oi7NXa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print predictions and true values\n",
        "num_examples = 10\n",
        "predicted_values = test_outputs[:num_examples].squeeze().numpy()\n",
        "true_values = y_test_tensor[:num_examples].squeeze().numpy()\n",
        "\n",
        "for i in range(num_examples):\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(f\"Predicted Value: {predicted_values[i]:.4f}\")\n",
        "    print(f\"True Value: {true_values[i]}\")\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8Tsr2g-JDMj",
        "outputId": "57a034a7-a374-486b-cd4a-26e67e1035d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1\n",
            "Predicted Value: 0.1695\n",
            "True Value: 0.0\n",
            "------\n",
            "Example 2\n",
            "Predicted Value: 0.3071\n",
            "True Value: 0.0\n",
            "------\n",
            "Example 3\n",
            "Predicted Value: 1.4919\n",
            "True Value: 2.0\n",
            "------\n",
            "Example 4\n",
            "Predicted Value: 0.2259\n",
            "True Value: 0.0\n",
            "------\n",
            "Example 5\n",
            "Predicted Value: 0.9322\n",
            "True Value: 1.0\n",
            "------\n",
            "Example 6\n",
            "Predicted Value: 0.1266\n",
            "True Value: 0.0\n",
            "------\n",
            "Example 7\n",
            "Predicted Value: 0.9608\n",
            "True Value: 1.0\n",
            "------\n",
            "Example 8\n",
            "Predicted Value: 1.9369\n",
            "True Value: 2.0\n",
            "------\n",
            "Example 9\n",
            "Predicted Value: 0.4734\n",
            "True Value: 1.0\n",
            "------\n",
            "Example 10\n",
            "Predicted Value: 1.4022\n",
            "True Value: 2.0\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seciton outputs the predicted class values as floating-point numbers, alongside their true class values. By observing the printed values, you can notice that the model provides predictions that are relatively close to the true values, indicating a satisfactory performance. However, there are some discrepancies, demonstrating potential areas for improvement in the model's accuracy."
      ],
      "metadata": {
        "id": "YC9ssV9HQh7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enhanced Neural Network Model**\n",
        "\n",
        "Given the foundation of this basic neural network, we can now work on enhancing the model by introducing additional layers and non-linear activation functions. This aims to capture more complex patterns in the data and improve the model's predictive performance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ADTepJrsZN4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Definition**\n",
        "\n",
        "Let's define the enhanced neural network model with three layers and ReLU activation functions."
      ],
      "metadata": {
        "id": "NMviD4WU3qGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(EnhancedNeuralNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "hidden_size1 = 10\n",
        "hidden_size2 = 10\n",
        "enhanced_model = EnhancedNeuralNet(input_size, hidden_size1, hidden_size2, output_size)"
      ],
      "metadata": {
        "id": "0rwpXGPc3o4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this enhanced model, the introduction of non-linear activation functions (ReLU) allow the network to capture more complex relationships in the data."
      ],
      "metadata": {
        "id": "RlAV_BKF4HZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the Enhanced Model**\n",
        "\n",
        "Now, the enhanced model can be trained, including L2 regularization and a learning rate scheduler."
      ],
      "metadata": {
        "id": "tnQh43oe4bul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.01  # L2 regularization\n",
        "epochs = 5000\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(enhanced_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)\n",
        "\n",
        "# Train the enhanced model\n",
        "for epoch in range(epochs):\n",
        "    outputs = enhanced_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if (epoch+1) % 500 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOweLgx54QQ2",
        "outputId": "9916b3de-a719-47b3-e562-3f2faec53e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [500/5000], Loss: 0.0090\n",
            "Epoch [1000/5000], Loss: 0.0087\n",
            "Epoch [1500/5000], Loss: 0.0084\n",
            "Epoch [2000/5000], Loss: 0.0084\n",
            "Epoch [2500/5000], Loss: 0.0084\n",
            "Epoch [3000/5000], Loss: 0.0084\n",
            "Epoch [3500/5000], Loss: 0.0069\n",
            "Epoch [4000/5000], Loss: 0.0068\n",
            "Epoch [4500/5000], Loss: 0.0061\n",
            "Epoch [5000/5000], Loss: 0.0061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, L2 regularization is used to prevent overfitting and a learning rate scheduler is used to adjust the learning rate during training, aiming to achieve better convergence."
      ],
      "metadata": {
        "id": "AM__-xfM4yJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating the Enhanced Model**\n",
        "\n",
        "Let's evaluate the enhanced model's performance on the test set."
      ],
      "metadata": {
        "id": "rYmwP9jx5kHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the enhanced model\n",
        "enhanced_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs_enhanced = enhanced_model(X_test_tensor)\n",
        "    test_loss_enhanced = criterion(test_outputs_enhanced, y_test_tensor)\n",
        "    print(f\"Enhanced Model Test Loss: {test_loss_enhanced.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "448CKN-X5COD",
        "outputId": "feef1f09-70ec-4308-ed64-9962ece973c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced Model Test Loss: 0.0141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is set to evaluation mode, and the test loss is computed to assess the enhanced model's performance."
      ],
      "metadata": {
        "id": "iWHzgIo26fG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predictions from Enhanced Model**\n",
        "\n",
        "Finally, let's compare the predictions from the enhanced model to the true values."
      ],
      "metadata": {
        "id": "2CqvJKbQ6tOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print predictions and true values from the enhanced model\n",
        "predicted_values_enhanced = test_outputs_enhanced[:num_examples].squeeze().numpy()\n",
        "for i in range(num_examples):\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(f\"Predicted Value (Enhanced): {predicted_values_enhanced[i]:.4f}\")\n",
        "    print(f\"True Value: {true_values[i]}\")\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J4EetMY6lOT",
        "outputId": "63e13d2c-f8d0-4d74-e9f7-00ad635ed21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1\n",
            "Predicted Value (Enhanced): 0.0031\n",
            "True Value: 0.0\n",
            "------\n",
            "Example 2\n",
            "Predicted Value (Enhanced): 0.0059\n",
            "True Value: 0.0\n",
            "------\n",
            "Example 3\n",
            "Predicted Value (Enhanced): 1.9167\n",
            "True Value: 2.0\n",
            "------\n",
            "Example 4\n",
            "Predicted Value (Enhanced): 0.0244\n",
            "True Value: 0.0\n",
            "------\n",
            "Example 5\n",
            "Predicted Value (Enhanced): 1.0435\n",
            "True Value: 1.0\n",
            "------\n",
            "Example 6\n",
            "Predicted Value (Enhanced): 0.0043\n",
            "True Value: 0.0\n",
            "------\n",
            "Example 7\n",
            "Predicted Value (Enhanced): 0.9385\n",
            "True Value: 1.0\n",
            "------\n",
            "Example 8\n",
            "Predicted Value (Enhanced): 1.9299\n",
            "True Value: 2.0\n",
            "------\n",
            "Example 9\n",
            "Predicted Value (Enhanced): 0.9299\n",
            "True Value: 1.0\n",
            "------\n",
            "Example 10\n",
            "Predicted Value (Enhanced): 1.7543\n",
            "True Value: 2.0\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section presents the predictions made by the enhanced neural network model alongside the true values for comparison. The predictions from the enhanced model are visibly more accurate and closer to the true values compared to the basic model, indicating an improvement in the model's performance."
      ],
      "metadata": {
        "id": "5LSgS9jj7HuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "\n",
        "This project provided an in-depth look at building, training, and evaluating neural networks using PyTorch, starting with a basic two-layer model and progressing to a more complex architecture. While the initial model showcased the essential steps in handling and processing a dataset for a regression task, its limitations highlighted the need for a more sophisticated approach. The enhanced neural network, featuring additional layers, non-linear activation functions, L2 regularization, and a learning rate scheduler, demonstrated a notable improvement in performance, as evident from the reduced loss on both training and test datasets.\n",
        "\n",
        "However, it is important to acknowledge that the Wine dataset, used for this project, is inherently categorical, suggesting that a classification approach might be more suitable. Despite this, the project effectively illustrates the capabilities of neural networks and the versatility of PyTorch, even when applied to a less conventional task.\n",
        "\n",
        "Future endeavors could explore classification models, compare their effectiveness with the developed regression models, and experiment with advanced neural network architectures and training techniques. Additionally, a comprehensive hyperparameter tuning could yield even better results and provide deeper insights into the model's behavior.\n"
      ],
      "metadata": {
        "id": "l2C8o1uqSMCo"
      }
    }
  ]
}